import * as tf from '@tensorflow/tfjs-node';
import config from '../config.json';
import {COLUMN_NAMES, SIZE_FEATURES} from './constants';
import {calculateFeatures} from './utils';

const BATCH_SIZE = 100;
const PERCENTAGE_USE_FOR_TESTING = 0.15; // 15%
const NUMBER_OF_REPETITIONS = 5;
// If the value is too big the loss will decrease too fast and start bouncing back to higher values. If it is too small
// it will take too long to decrease the loss and it will not be accurate. The more data we have the lower this value can be
const LEARNING_RATE = 0.000000001;
// It is in charge of trying different importance (or weights) for the different inputs (features)
const OPTIMIZER = tf.train.adam(LEARNING_RATE);
// Algorithm that calculates a number (the loss) indicating how bad the model's prediction was on a single example.
// We want to decrease the loss. There are two main types of machine learning problems, categorization
// (discrete values like a property type or true or false) and
// regressions (where the output is any real number, like the price of a house)
// We need to pick a loss function for regressions
const LOSS = tf.losses.huberLoss;

export const generateModel = async (sizeOfDataSet: number) => {

    const csvDataset = tf.data.csv(
        `file://${config.pathLandRegistryCsv}`, {
            hasHeader: false,
            columnNames: COLUMN_NAMES,
            columnConfigs: {
                price: {
                    required: true,
                    dtype: 'int32',
                    isLabel: true // Indicates it's the output
                },
                postcode: {
                    dtype: 'string'
                },
                date: {
                    dtype: 'string'
                },
                propertyType: {
                    dtype: 'string'
                },
                oldNew: {
                    dtype: 'string'
                }
            },
            configuredColumnsOnly: true
        });

    const sizeOfTestDataSet = Math.round(sizeOfDataSet * PERCENTAGE_USE_FOR_TESTING);

    const mapVariablesToFeatures = ({xs, ys}: any) => {
        const date = new Date(xs.date);
        const xsValue = calculateFeatures(xs.postcode, xs.propertyType, xs.oldNew, date);
        return { xs: xsValue, ys: Object.values(ys) };
    };

    const filterRequiredValues = ({xs}: any) => {
        return xs.postcode && xs.postcode.length > 0;
    };

    // @ts-ignore
    const dataSet = csvDataset.filter(filterRequiredValues).map(mapVariablesToFeatures)
        .shuffle(sizeOfTestDataSet); // increase to make it faster

    const testDataSet = dataSet.take(sizeOfTestDataSet).batch(BATCH_SIZE);
    const trainingDataSet = dataSet.skip(sizeOfTestDataSet).batch(BATCH_SIZE);
    console.log(`Size of the data set: ${sizeOfDataSet}`);

    // When training a model, we are trying to assign importance (or weights) to the different inputs (or features)
    // This weights are generated by the machine learning algorithm. To create more complex models (that are more accurate)
    // multiples internal layers can be defined. The values will flow from one layer to the next and the algorithm
    // will generate weights for all of them.
    // Since this is a regression problem the activation (the function that applies the weight to pass it to the next layer)
    // also needs to be one that does not categorize
    // There are some suggestions to pick the same number or layers as the number of features as a starting point
    const model = tf.sequential();
    model.add(tf.layers.dense({ units: 300, activation: 'linear', inputShape: [SIZE_FEATURES] }));
    model.add(tf.layers.dense({ units: 300, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 300, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 300, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 300, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 300, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 300, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 300, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 300, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 300, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 200, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 100, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 50, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 10, activation: 'linear' }));
    model.add(tf.layers.dense({ units: 1 }));


    model.compile({
        optimizer: OPTIMIZER,
        loss: LOSS
    });

    await model.fitDataset(trainingDataSet, {
        epochs: NUMBER_OF_REPETITIONS,
        validationData: testDataSet,
        verbose: 1
    });

    await model.save(`file://${config.pathToModel}`);

    console.log('Model has finished training');

    return model;
};
